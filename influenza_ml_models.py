
# influenza_ml_models.py
# Unsupervised ML analysis of influenza HA protein features:
# - Loads HA_features.csv generated by influenza_project_pipeline.py
# - Performs PCA (dimensionality reduction)
# - Performs k-means clustering
# - Creates visualisations (saved as PNG files)
#
# This script is designed to:
# 1. Demonstrate data science methods on biological sequence data.
# 2. Support analysis of influenza virus evolution over time.
# 3. Produce figures that can be used in your MSc project report.

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use("Agg")  # non-interactive backend for script execution
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from scipy.cluster.hierarchy import linkage, dendrogram
import os

# -------------------------
# CONFIGURATION
# -------------------------

FEATURES_CSV = "HA_features.csv"   # input from previous pipeline
OUTPUT_DIR = "ml_outputs"          # folder for figures and outputs
os.makedirs(OUTPUT_DIR, exist_ok=True)

# -------------------------
# LOAD FEATURES
# -------------------------

print("Loading feature dataset...")
df = pd.read_csv(FEATURES_CSV)

print(f"Original feature shape: {df.shape}")
print("First few rows:")
print(df.head())

# -------------------------
# FILTER DATA (OPTIONAL: YEARS >= 2010)
# -------------------------

# Focus on more recent evolution, as guided by the supervisor.
df_recent = df.copy()
df_recent = df_recent[df_recent["Year"].notna()]
df_recent["Year"] = df_recent["Year"].astype(int)

# If you want to restrict to 2010 onwards, uncomment this line:
# df_recent = df_recent[df_recent["Year"] >= 2010]

print(f"Filtered feature shape (non-null years): {df_recent.shape}")
years = df_recent["Year"].values

# -------------------------
# SELECT NUMERIC FEATURES FOR ML
# -------------------------

# Use all columns except Accession and Year as numeric features
feature_cols = [c for c in df_recent.columns if c not in ["Accession", "Year"]]
X = df_recent[feature_cols].values

print(f"Using {len(feature_cols)} numeric features for ML:")
print(feature_cols)

# -------------------------
# STANDARDISE FEATURES
# -------------------------

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------
# PCA ANALYSIS
# -------------------------

print("Running PCA...")
n_components = min(10, X_scaled.shape[1])  # cap to 10 for readability
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

explained_var = pca.explained_variance_ratio_
print("Explained variance ratio by PC:")
for i, var in enumerate(explained_var, start=1):
    print(f"  PC{i}: {var:.4f}")

# Scree plot (explained variance)
pcs = np.arange(1, len(explained_var) + 1)
plt.figure()
plt.bar(pcs, explained_var, color="steelblue", alpha=0.8, label="Explained variance")
plt.plot(pcs, np.cumsum(explained_var), marker="o", color="orange", label="Cumulative")
plt.xlabel("Principal Component")
plt.ylabel("Explained Variance Ratio")
plt.title("PCA Scree Plot")
plt.xticks(pcs)
plt.legend()
plt.tight_layout()
scree_path = os.path.join(OUTPUT_DIR, "pca_scree_plot.png")
plt.savefig(scree_path, dpi=300)
plt.close()
print(f"Scree plot saved to {scree_path}")

# Save PCA-transformed data (first three components) with metadata
pca_cols = [f"PC{i}" for i in range(1, X_pca.shape[1] + 1)]
df_pca = pd.DataFrame(X_pca, columns=pca_cols)
df_pca["Year"] = years
df_pca["Accession"] = df_recent["Accession"].values
df_pca.to_csv(os.path.join(OUTPUT_DIR, "HA_PCA_scores.csv"), index=False)
print("Saved PCA scores to HA_PCA_scores.csv")

# -------------------------
# PLOT: PCA COLOURED BY YEAR
# -------------------------

print("Creating PCA scatter plot coloured by year...")
plt.figure()
scatter = plt.scatter(df_pca["PC1"], df_pca["PC2"], c=df_pca["Year"], alpha=0.7)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of HA Features (coloured by Year)")
cbar = plt.colorbar(scatter)
cbar.set_label("Year")
plt.tight_layout()
pca_year_plot = os.path.join(OUTPUT_DIR, "pca_by_year.png")
plt.savefig(pca_year_plot, dpi=300)
plt.close()
print(f"PCA by year plot saved to {pca_year_plot}")

# PLOT: Additional PCA projections (PC1–PC3 and PC2–PC3) coloured by year
if "PC3" in df_pca.columns:
    plt.figure()
    scatter = plt.scatter(df_pca["PC1"], df_pca["PC3"], c=df_pca["Year"], alpha=0.7)
    plt.xlabel("PC1")
    plt.ylabel("PC3")
    plt.title("PCA PC1 vs PC3 (coloured by Year)")
    cbar = plt.colorbar(scatter)
    cbar.set_label("Year")
    plt.tight_layout()
    pc13_plot = os.path.join(OUTPUT_DIR, "pca_pc1_pc3_by_year.png")
    plt.savefig(pc13_plot, dpi=300)
    plt.close()
    print(f"PCA PC1–PC3 plot saved to {pc13_plot}")

    plt.figure()
    scatter = plt.scatter(df_pca["PC2"], df_pca["PC3"], c=df_pca["Year"], alpha=0.7)
    plt.xlabel("PC2")
    plt.ylabel("PC3")
    plt.title("PCA PC2 vs PC3 (coloured by Year)")
    cbar = plt.colorbar(scatter)
    cbar.set_label("Year")
    plt.tight_layout()
    pc23_plot = os.path.join(OUTPUT_DIR, "pca_pc2_pc3_by_year.png")
    plt.savefig(pc23_plot, dpi=300)
    plt.close()
    print(f"PCA PC2–PC3 plot saved to {pc23_plot}")

# -------------------------
# K-MEANS CLUSTERING
# -------------------------

print("Running k-means clustering...")
k = 4  # chosen for demonstration; can be tuned
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

df_pca["Cluster"] = clusters
df_pca.to_csv(os.path.join(OUTPUT_DIR, "HA_PCA_with_clusters.csv"), index=False)
print("Saved PCA scores with cluster labels to HA_PCA_with_clusters.csv")

# Cluster centroid summaries (back-transformed to original feature scales)
cluster_sizes = pd.Series(clusters).value_counts().sort_index()
centers_original = pd.DataFrame(
    scaler.inverse_transform(kmeans.cluster_centers_),
    columns=feature_cols
)
centers_original.insert(0, "Cluster", centers_original.index)
centers_original["Count"] = centers_original["Cluster"].map(cluster_sizes)
centroid_path = os.path.join(OUTPUT_DIR, "cluster_centroids.csv")
centers_original.to_csv(centroid_path, index=False)
print(f"Cluster centroids saved to {centroid_path}")

# Silhouette analysis
sil_score = silhouette_score(X_scaled, clusters)
sil_values = silhouette_samples(X_scaled, clusters)
df_pca["Silhouette"] = sil_values
print(f"Overall silhouette score: {sil_score:.3f}")

# Silhouette plot
plt.figure(figsize=(6, 5))
y_lower = 10
for i in range(k):
    vals = np.sort(sil_values[clusters == i])
    size = vals.shape[0]
    y_upper = y_lower + size
    color = plt.cm.tab10(i)
    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, vals, facecolor=color, alpha=0.7)
    plt.text(-0.1, y_lower + 0.5 * size, str(i))
    y_lower = y_upper + 10

plt.axvline(sil_score, color="red", linestyle="--", label=f"Mean silhouette = {sil_score:.2f}")
plt.xlabel("Silhouette coefficient")
plt.ylabel("Sample index within cluster")
plt.title("Silhouette plot for k-means clusters")
plt.legend()
plt.tight_layout()
sil_plot = os.path.join(OUTPUT_DIR, "silhouette_plot.png")
plt.savefig(sil_plot, dpi=300)
plt.close()
print(f"Silhouette plot saved to {sil_plot}")

# PLOT: PCA COLOURED BY CLUSTER
print("Creating PCA scatter plot coloured by cluster...")
plt.figure()
scatter = plt.scatter(df_pca["PC1"], df_pca["PC2"], c=df_pca["Cluster"], cmap="tab10", alpha=0.7)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of HA Features (k-means clusters)")
cbar = plt.colorbar(scatter)
cbar.set_label("Cluster")
plt.tight_layout()
pca_cluster_plot = os.path.join(OUTPUT_DIR, "pca_by_cluster.png")
plt.savefig(pca_cluster_plot, dpi=300)
plt.close()
print(f"PCA by cluster plot saved to {pca_cluster_plot}")

# -------------------------
# HIERARCHICAL CLUSTERING (SUBSET)
# -------------------------

print("Running hierarchical clustering on a subset (for legible dendrogram)...")

# Use a subset to keep the dendrogram readable
max_samples = 100
if df_recent.shape[0] > max_samples:
    df_subset = df_recent.sample(n=max_samples, random_state=42)
else:
    df_subset = df_recent.copy()

X_subset = df_subset[feature_cols].values
subset_scaler = StandardScaler()
X_subset_scaled = subset_scaler.fit_transform(X_subset)

# Perform hierarchical clustering
linked = linkage(X_subset_scaled, method="ward")

plt.figure(figsize=(10, 6))
dendrogram(linked,
           labels=df_subset["Year"].astype(str).values,
           leaf_rotation=90,
           leaf_font_size=6)
plt.title("Hierarchical clustering dendrogram (subset of HA sequences)")
plt.xlabel("Sample (Year)")
plt.ylabel("Distance")
plt.tight_layout()
dendro_plot = os.path.join(OUTPUT_DIR, "hierarchical_dendrogram.png")
plt.savefig(dendro_plot, dpi=300)
plt.close()
print(f"Dendrogram plot saved to {dendro_plot}")

print("ML analysis script completed successfully.")
